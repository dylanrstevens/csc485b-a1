{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Load the extension that allows us to compile CUDA code in python notebooks\n# Documentation is here: https://nvcc4jupyter.readthedocs.io/en/latest/\n!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git\n%load_ext nvcc4jupyter","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dclGnLGAgbtH","outputId":"5d326cc4-09ee-4f40-bded-3b3c2e2f3d94","execution":{"iopub.status.busy":"2024-10-02T20:16:25.918278Z","iopub.execute_input":"2024-10-02T20:16:25.918589Z","iopub.status.idle":"2024-10-02T20:18:19.325975Z","shell.execute_reply.started":"2024-10-02T20:16:25.918555Z","shell.execute_reply":"2024-10-02T20:18:19.324839Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-zfzr55mz\n  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-zfzr55mz\n  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit 28f872a2f99a1b201bcd0db14fdbc5a496b9bfd7\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: nvcc4jupyter\n  Building wheel for nvcc4jupyter (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for nvcc4jupyter: filename=nvcc4jupyter-1.2.1-py3-none-any.whl size=10743 sha256=3a570d3ab74e9051a615aa78d54722c78a69a0c83a1d407fccde14c37a30e96c\n  Stored in directory: /tmp/pip-ephem-wheel-cache-rspde61h/wheels/a8/b9/18/23f8ef71ceb0f63297dd1903aedd067e6243a68ea756d6feea\nSuccessfully built nvcc4jupyter\nInstalling collected packages: nvcc4jupyter\nSuccessfully installed nvcc4jupyter-1.2.1\nDetected platform \"Kaggle\". Running its setup...\nUpdating the package lists...\nInstalling nvidia-cuda-toolkit, this may take a few minutes...\nSource files will be saved in \"/tmp/tmpb53575v7\".\n","output_type":"stream"}]},{"cell_type":"code","source":"%%cuda_group_save -g \"source\" -n \"data_types.h\"\n/**\n * A collection of commonly used data types throughout this project.\n */\n#pragma once\n\n#include <stdint.h> // uint32_t\n\nusing element_t = uint32_t;","metadata":{"id":"VVbDQthwogQF","execution":{"iopub.status.busy":"2024-10-02T20:18:19.328321Z","iopub.execute_input":"2024-10-02T20:18:19.328819Z","iopub.status.idle":"2024-10-02T20:18:19.335011Z","shell.execute_reply.started":"2024-10-02T20:18:19.328766Z","shell.execute_reply":"2024-10-02T20:18:19.333726Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"%%cuda_group_save -g \"source\" -n \"cuda_common.h\"\n/**\n * Standard macros that can be useful for error checking.\n * https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__ERROR.html\n */\n#pragma once\n\n#include <cuda.h>\n\n#define CUDA_CALL(exp)                                       \\\n    do {                                                     \\\n        cudaError res = (exp);                               \\\n        if(res != cudaSuccess) {                             \\\n            printf(\"Error at %s:%d\\n %s\\n\",                  \\\n                __FILE__,__LINE__, cudaGetErrorString(res)); \\\n           exit(EXIT_FAILURE);                               \\\n        }                                                    \\\n    } while(0)\n\n#define CHECK_ERROR(msg)                                             \\\n    do {                                                             \\\n        cudaError_t err = cudaGetLastError();                        \\\n        if(cudaSuccess != err) {                                     \\\n            printf(\"Error (%s) at %s:%d\\n %s\\n\",                     \\\n                (msg), __FILE__, __LINE__, cudaGetErrorString(err)); \\\n            exit(EXIT_FAILURE);                                      \\\n        }                                                            \\\n    } while (0)","metadata":{"id":"ZqET4uI2ggwf","execution":{"iopub.status.busy":"2024-10-02T20:18:19.336152Z","iopub.execute_input":"2024-10-02T20:18:19.336518Z","iopub.status.idle":"2024-10-02T20:18:19.345744Z","shell.execute_reply.started":"2024-10-02T20:18:19.336476Z","shell.execute_reply":"2024-10-02T20:18:19.344979Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"%%cuda_group_save -g \"source\" -n \"data_generator.h\"\n/**\n * Functions for generating random input data with a fixed seed\n */\n#pragma once\n\n#include <random>  // for std::mt19937, std::uniform_int_distribution\n#include <vector>\n\n#include \"data_types.h\"\n\nnamespace csc485b {\nnamespace a1 {\n\n/**\n * Generates and returns a vector of random uniform data of a given length, n,\n * for any integral type. Input range will be [0, 2n].\n */\ntemplate < typename T >\nstd::vector< T > generate_uniform( std::size_t n )\n{\n    // for details of random number generation, see:\n    // https://en.cppreference.com/w/cpp/numeric/random/uniform_int_distribution\n    std::size_t random_seed = 20240916;  // use magic seed\n    std::mt19937 rng( random_seed );     // use mersenne twister generator\n    std::uniform_int_distribution<> distrib(0, 2 * n);\n\n    std::vector< T > random_data( n ); // init array\n    std::generate( std::begin( random_data )\n                 , std::end  ( random_data )\n                 , [ &rng, &distrib ](){ return static_cast< T >( distrib( rng ) ); });\n\n    return random_data;\n}\n\n} // namespace a1\n} // namespace csc485b","metadata":{"id":"GY0L7rKhoVaZ","execution":{"iopub.status.busy":"2024-10-02T20:18:19.348025Z","iopub.execute_input":"2024-10-02T20:18:19.348318Z","iopub.status.idle":"2024-10-02T20:18:19.355029Z","shell.execute_reply.started":"2024-10-02T20:18:19.348287Z","shell.execute_reply":"2024-10-02T20:18:19.354209Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"%%cuda_group_save -g \"source\" -n \"algorithm_choices.h\"\n#pragma once\n\n#include <vector>\n\n#include \"data_types.h\"\n\nnamespace csc485b {\nnamespace a1 {\nnamespace cpu {\n\nvoid run_cpu_baseline( std::vector< element_t > data, std::size_t switch_at, std::size_t n );\n\n} // namespace cpu\n\n\nnamespace gpu {\n\nvoid run_gpu_soln( std::vector< element_t > data, std::size_t switch_at, std::size_t n );\n\n} // namespace gpu\n} // namespace a1\n} // namespace csc485b","metadata":{"id":"IJOKRZuCkDh2","execution":{"iopub.status.busy":"2024-10-02T20:18:19.356358Z","iopub.execute_input":"2024-10-02T20:18:19.356678Z","iopub.status.idle":"2024-10-02T20:18:19.366292Z","shell.execute_reply.started":"2024-10-02T20:18:19.356640Z","shell.execute_reply":"2024-10-02T20:18:19.365490Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"%%cuda_group_save -g \"source\" -n \"cpu_baseline.cu\"\n/**\n * CPU methods that the GPU should outperform.\n */\n\n#include \"algorithm_choices.h\"\n\n#include <algorithm> // std::sort()\n#include <chrono>    // for timing\n#include <iostream>  // std::cout, std::endl\n\nnamespace csc485b {\nnamespace a1      {\nnamespace cpu     {\n\n/**\n * Simple solution that just sorts the whole array with a built-in sort\n * function and then resorts the last portion in the opposing order with\n * a second call to that same built-in sort function.\n */\nvoid opposing_sort( element_t * data, std::size_t invert_at_pos, std::size_t num_elements )\n{\n    std::sort( data, data + num_elements, std::less< element_t >{} );\n    std::sort( data + invert_at_pos, data + num_elements, std::greater< element_t >{} );\n}\n\n/**\n * Run the single-threaded CPU baseline that students are supposed to outperform\n * in order to obtain higher grades on this assignment. Times the execution and\n * prints to the standard output (e.g., the screen) that \"wall time.\" Note that\n * the functions takes the input by value so as to not perturb the original data\n * in place.\n */\nvoid run_cpu_baseline( std::vector< element_t > data, std::size_t switch_at, std::size_t n )\n{\n    auto const cpu_start = std::chrono::high_resolution_clock::now();\n    opposing_sort( data.data(), switch_at, n );\n    auto const cpu_end = std::chrono::high_resolution_clock::now();\n\n    std::cout << \"CPU Baseline time: \"\n              << std::chrono::duration_cast<std::chrono::nanoseconds>(cpu_end - cpu_start).count()\n              << \" ns\" << std::endl;\n\n    for( auto const x : data ) std::cout << x << \" \"; std::cout << std::endl;\n}\n\n} // namespace cpu\n} // namespace a1\n} // namespace csc485b","metadata":{"id":"V3lAuiBEhKjc","execution":{"iopub.status.busy":"2024-10-02T20:18:19.367424Z","iopub.execute_input":"2024-10-02T20:18:19.367742Z","iopub.status.idle":"2024-10-02T20:18:19.375142Z","shell.execute_reply.started":"2024-10-02T20:18:19.367711Z","shell.execute_reply":"2024-10-02T20:18:19.374311Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"%%cuda_group_save -g \"source\" -n \"gpu_solution.cu\"\n/**\n * The file in which you will implement your GPU solutions!\n */\n\n#include \"algorithm_choices.h\"\n\n#include <chrono>    // for timing\n#include <iostream>  // std::cout, std::endl\n\n#include \"cuda_common.h\"\n\nnamespace csc485b {\nnamespace a1      {\nnamespace gpu     {\n\n/**\n * The CPU baseline benefits from warm caches because the data was generated on\n * the CPU. Run the data through the GPU once with some arbitrary logic to\n * ensure that the GPU cache is warm too and the comparison is more fair.\n */\n__global__\nvoid warm_the_gpu( element_t * data, std::size_t invert_at_pos, std::size_t num_elements )\n{\n    int const th_id = blockIdx.x * blockDim.x + threadIdx.x;\n\n    // We know this will never be true, because of the data generator logic,\n    // but I doubt that the compiler will figure it out. Thus every element\n    // should be read, but none of them should be modified.\n    if( th_id < num_elements && data[ th_id ] > num_elements * 100 )\n    {\n        ++data[ th_id ]; // should not be possible.\n    }\n}\n\n/**\n * Your solution. Should match the CPU output.\n */\n\n__global__\nvoid opposing_sort(element_t * data, std::size_t invert_at_pos, std::size_t num_elements)\n{\n    // Calculate the next power of two for the given input size\n    std::size_t padded_size = 1;\n    while (padded_size < num_elements) {\n        padded_size <<= 1;\n    }\n\n    // Step 1: Allocate shared memory with padding\n    __shared__ element_t shared_data[1024];\n    \n    // Initialize padded value\n    element_t pad_value = 1e9; // Use appropriate padding value\n\n    // Compute the global thread ID and local thread ID within the block\n    int const th_id = blockIdx.x * blockDim.x + threadIdx.x;\n    int const local_th_id = threadIdx.x;\n    \n    // Step 2: Load data from global memory to shared memory\n    if (th_id < padded_size) {\n        if (th_id < num_elements) {\n            shared_data[local_th_id] = data[th_id]; // Load actual data\n        } else {\n            shared_data[local_th_id] = pad_value; // Load padding value\n        }\n    }\n    \n    // Ensure all threads in the block have loaded their data into shared memory\n    __syncthreads();\n    \n    // Step 3: Perform bitonic sort using shared memory\n    if (th_id < padded_size) {\n        for (int k = 2; k <= padded_size; k <<= 1) {\n            for (int j = k >> 1; j > 0; j >>= 1) {\n                int i_swap = local_th_id ^ j;\n\n                if (local_th_id < i_swap && i_swap < blockDim.x) {\n                    // Standard bitonic sort logic\n                    if ((k != padded_size) || ((k == padded_size) && (padded_size != num_elements))) {\n                        if ((local_th_id & k) == 0) {\n                            if (shared_data[local_th_id] > shared_data[i_swap]) {\n                                element_t t = shared_data[local_th_id];\n                                shared_data[local_th_id] = shared_data[i_swap];\n                                shared_data[i_swap] = t;\n                            }\n                        } else {\n                            if (shared_data[local_th_id] < shared_data[i_swap]) {\n                                element_t t = shared_data[local_th_id];\n                                shared_data[local_th_id] = shared_data[i_swap];\n                                shared_data[i_swap] = t;\n                            }\n                        }\n                    }\n                    // Handle the last step where we apply the inversion at the specified position ONLY for inputs where the size is a power of 2\n                    else if (padded_size == num_elements) {\n                        if (local_th_id < invert_at_pos) {\n                            if (shared_data[local_th_id] > shared_data[i_swap]) {\n                                element_t t = shared_data[local_th_id];\n                                shared_data[local_th_id] = shared_data[i_swap];\n                                shared_data[i_swap] = t;\n                            }\n                        } else if (local_th_id >= invert_at_pos) {\n                            if (shared_data[local_th_id] < shared_data[i_swap]) {\n                                element_t t = shared_data[local_th_id];\n                                shared_data[local_th_id] = shared_data[i_swap];\n                                shared_data[i_swap] = t;\n                            }\n                        }\n                    }\n                }\n                \n                // Synchronize threads within the block before the next stage\n                __syncthreads();\n            }\n        }\n    }\n    // Step 3.5\n    // If the input size is not a power of 2, then sort the array normally, and reverse elements after 3n/4\n    if (padded_size > num_elements) {\n        if ((local_th_id >= invert_at_pos) && (local_th_id < num_elements)) {\n            int swap_partner = num_elements - (local_th_id - invert_at_pos) - 1;\n            if (local_th_id < swap_partner) {\n                if (shared_data[local_th_id] < shared_data[swap_partner]) {\n                    element_t t = shared_data[local_th_id];\n                    shared_data[local_th_id] = shared_data[swap_partner];\n                    shared_data[swap_partner] = t;\n                }\n            }\n        }\n    }\n    __syncthreads();\n    \n    // Step 4: Copy sorted data back to global memory, excluding padding\n    if (th_id < num_elements) {\n        data[th_id] = shared_data[local_th_id];\n    }\n}\n\n\n\n/**\n * Performs all the logic of allocating device vectors and copying host/input\n * vectors to the device. Times the opposing_sort() kernel with wall time,\n * but excludes set up and tear down costs such as mallocs, frees, and memcpies.\n */\nvoid run_gpu_soln( std::vector< element_t > data, std::size_t switch_at, std::size_t n )\n{\n    // Kernel launch configurations. Feel free to change these.\n    // This is set to maximise the size of a thread block on a T4, but it hasn't\n    // been tuned. It's not known if this is optimal.\n    std::size_t const threads_per_block = 1024;\n    std::size_t const num_blocks =  ( n + threads_per_block - 1 ) / threads_per_block;\n    \n    // Allocate arrays on the device/GPU\n    element_t * d_data;\n    cudaMalloc( (void**) & d_data, sizeof( element_t ) * n );\n    CHECK_ERROR(\"Allocating input array on device\");\n\n    // Copy the input from the host to the device/GPU\n    cudaMemcpy( d_data, data.data(), sizeof( element_t ) * n, cudaMemcpyHostToDevice );\n    CHECK_ERROR(\"Copying input array to device\");\n\n    // Warm the cache on the GPU for a more fair comparison\n    warm_the_gpu<<< num_blocks, threads_per_block>>>( d_data, switch_at, n );\n\n    // Time the execution of the kernel that you implemented\n    auto const kernel_start = std::chrono::high_resolution_clock::now();\n    opposing_sort<<< num_blocks, threads_per_block>>>( d_data, switch_at, n );\n    auto const kernel_end = std::chrono::high_resolution_clock::now();\n    CHECK_ERROR(\"Executing kernel on device\");\n\n    // After the timer ends, copy the result back, free the device vector,\n    // and echo out the timings and the results.\n    cudaMemcpy( data.data(), d_data, sizeof( element_t ) * n, cudaMemcpyDeviceToHost );\n    CHECK_ERROR(\"Transferring result back to host\");\n    cudaFree( d_data );\n    CHECK_ERROR(\"Freeing device memory\");\n\n    std::cout << \"GPU Solution time: \"\n              << std::chrono::duration_cast<std::chrono::nanoseconds>(kernel_end - kernel_start).count()\n              << \" ns\" << std::endl;\n\n    for( auto const x : data ) std::cout << x << \" \"; std::cout << std::endl;\n}\n\n} // namespace gpu\n} // namespace a1\n} // namespace csc485b","metadata":{"id":"bjTbQ3EO2NwQ","execution":{"iopub.status.busy":"2024-10-02T22:36:27.084772Z","iopub.execute_input":"2024-10-02T22:36:27.085178Z","iopub.status.idle":"2024-10-02T22:36:27.097804Z","shell.execute_reply.started":"2024-10-02T22:36:27.085140Z","shell.execute_reply":"2024-10-02T22:36:27.096531Z"},"trusted":true},"execution_count":157,"outputs":[]},{"cell_type":"code","source":"%%cuda_group_save -g \"source\" -n \"main.cu\"\n/**\n * Driver for the benchmark comparison. Generates random data,\n * runs the CPU baseline, and then runs your code.\n */\n\n#include <cstddef>  // std::size_t type\n#include <iostream> // std::cout, std::endl\n#include <vector>\n\n#include \"algorithm_choices.h\"\n#include \"data_generator.h\"\n#include \"data_types.h\"\n#include \"cuda_common.h\"\n\nint main()\n{\n    std::size_t const n = 25;\n    std::size_t const switch_at = 3 * ( n >> 2 ) ;\n\n    auto data = csc485b::a1::generate_uniform< element_t >( n );\n    /*\n    for (auto &d:data) {\n        std::cout << d << ' ';\n    }\n    */\n    std::cout << '\\n';\n    csc485b::a1::cpu::run_cpu_baseline( data, switch_at, n );\n    csc485b::a1::gpu::run_gpu_soln( data, switch_at, n );\n\n    return EXIT_SUCCESS;\n}","metadata":{"id":"IRvVeK-QifnZ","execution":{"iopub.status.busy":"2024-10-02T22:37:17.701965Z","iopub.execute_input":"2024-10-02T22:37:17.702675Z","iopub.status.idle":"2024-10-02T22:37:17.708263Z","shell.execute_reply.started":"2024-10-02T22:37:17.702633Z","shell.execute_reply":"2024-10-02T22:37:17.707205Z"},"trusted":true},"execution_count":162,"outputs":[]},{"cell_type":"code","source":"%cuda_group_run --group \"source\" --compiler-args \"-O0 -g -std=c++20 -arch=sm_75\"","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S7F0eVsGjUNp","outputId":"310fd1bb-bd82-4238-d721-f6637f546255","execution":{"iopub.status.busy":"2024-10-02T22:37:21.661021Z","iopub.execute_input":"2024-10-02T22:37:21.661377Z","iopub.status.idle":"2024-10-02T22:37:32.255834Z","shell.execute_reply.started":"2024-10-02T22:37:21.661342Z","shell.execute_reply":"2024-10-02T22:37:32.254908Z"},"trusted":true},"execution_count":163,"outputs":[{"name":"stdout","text":"\nCPU Baseline time: 3783 ns\n0 0 1 1 5 6 8 8 10 10 10 15 18 21 24 26 27 27 47 44 41 40 40 31 28 \nth_id; 18\nth_id; 19\nth_id; 20\nswap_partner; 24\nswap_partner; 23\nswap_partner; 22\nGPU Solution time: 57065 ns\n0 0 1 1 5 6 8 8 10 10 10 15 18 21 24 26 27 27 47 44 41 40 40 31 28 \n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"id":"K0Yqomwu6WsP"},"execution_count":null,"outputs":[]}]}