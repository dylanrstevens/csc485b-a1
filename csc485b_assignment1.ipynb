{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dclGnLGAgbtH",
        "outputId": "c5ef9ddf-8338-4013-c049-6fc414fef767"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-me7ocbbw\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-me7ocbbw\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit 28f872a2f99a1b201bcd0db14fdbc5a496b9bfd7\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "The nvcc4jupyter extension is already loaded. To reload it, use:\n",
            "  %reload_ext nvcc4jupyter\n"
          ]
        }
      ],
      "source": [
        "# Load the extension that allows us to compile CUDA code in python notebooks\n",
        "# Documentation is here: https://nvcc4jupyter.readthedocs.io/en/latest/\n",
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
        "%load_ext nvcc4jupyter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVbDQthwogQF"
      },
      "outputs": [],
      "source": [
        "%%cuda_group_save -g \"source\" -n \"data_types.h\"\n",
        "/**\n",
        " * A collection of commonly used data types throughout this project.\n",
        " */\n",
        "#pragma once\n",
        "\n",
        "#include <stdint.h> // uint32_t\n",
        "\n",
        "using element_t = uint32_t;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqET4uI2ggwf"
      },
      "outputs": [],
      "source": [
        "%%cuda_group_save -g \"source\" -n \"cuda_common.h\"\n",
        "/**\n",
        " * Standard macros that can be useful for error checking.\n",
        " * https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__ERROR.html\n",
        " */\n",
        "#pragma once\n",
        "\n",
        "#include <cuda.h>\n",
        "\n",
        "#define CUDA_CALL(exp)                                       \\\n",
        "    do {                                                     \\\n",
        "        cudaError res = (exp);                               \\\n",
        "        if(res != cudaSuccess) {                             \\\n",
        "            printf(\"Error at %s:%d\\n %s\\n\",                  \\\n",
        "                __FILE__,__LINE__, cudaGetErrorString(res)); \\\n",
        "           exit(EXIT_FAILURE);                               \\\n",
        "        }                                                    \\\n",
        "    } while(0)\n",
        "\n",
        "#define CHECK_ERROR(msg)                                             \\\n",
        "    do {                                                             \\\n",
        "        cudaError_t err = cudaGetLastError();                        \\\n",
        "        if(cudaSuccess != err) {                                     \\\n",
        "            printf(\"Error (%s) at %s:%d\\n %s\\n\",                     \\\n",
        "                (msg), __FILE__, __LINE__, cudaGetErrorString(err)); \\\n",
        "            exit(EXIT_FAILURE);                                      \\\n",
        "        }                                                            \\\n",
        "    } while (0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GY0L7rKhoVaZ"
      },
      "outputs": [],
      "source": [
        "%%cuda_group_save -g \"source\" -n \"data_generator.h\"\n",
        "/**\n",
        " * Functions for generating random input data with a fixed seed\n",
        " */\n",
        "#pragma once\n",
        "\n",
        "#include <random>  // for std::mt19937, std::uniform_int_distribution\n",
        "#include <vector>\n",
        "\n",
        "#include \"data_types.h\"\n",
        "\n",
        "namespace csc485b {\n",
        "namespace a1 {\n",
        "\n",
        "/**\n",
        " * Generates and returns a vector of random uniform data of a given length, n,\n",
        " * for any integral type. Input range will be [0, 2n].\n",
        " */\n",
        "template < typename T >\n",
        "std::vector< T > generate_uniform( std::size_t n )\n",
        "{\n",
        "    // for details of random number generation, see:\n",
        "    // https://en.cppreference.com/w/cpp/numeric/random/uniform_int_distribution\n",
        "    std::size_t random_seed = 20240916;  // use magic seed\n",
        "    std::mt19937 rng( random_seed );     // use mersenne twister generator\n",
        "    std::uniform_int_distribution<> distrib(0, 2 * n);\n",
        "\n",
        "    std::vector< T > random_data( n ); // init array\n",
        "    std::generate( std::begin( random_data )\n",
        "                 , std::end  ( random_data )\n",
        "                 , [ &rng, &distrib ](){ return static_cast< T >( distrib( rng ) ); });\n",
        "\n",
        "    return random_data;\n",
        "}\n",
        "\n",
        "} // namespace a1\n",
        "} // namespace csc485b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJOKRZuCkDh2"
      },
      "outputs": [],
      "source": [
        "%%cuda_group_save -g \"source\" -n \"algorithm_choices.h\"\n",
        "#pragma once\n",
        "\n",
        "#include <vector>\n",
        "\n",
        "#include \"data_types.h\"\n",
        "\n",
        "namespace csc485b {\n",
        "namespace a1 {\n",
        "namespace cpu {\n",
        "\n",
        "void run_cpu_baseline( std::vector< element_t > data, std::size_t switch_at, std::size_t n );\n",
        "\n",
        "} // namespace cpu\n",
        "\n",
        "\n",
        "namespace gpu {\n",
        "\n",
        "void run_gpu_soln( std::vector< element_t > data, std::size_t switch_at, std::size_t n );\n",
        "\n",
        "} // namespace gpu\n",
        "} // namespace a1\n",
        "} // namespace csc485b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3lAuiBEhKjc"
      },
      "outputs": [],
      "source": [
        "%%cuda_group_save -g \"source\" -n \"cpu_baseline.cu\"\n",
        "/**\n",
        " * CPU methods that the GPU should outperform.\n",
        " */\n",
        "\n",
        "#include \"algorithm_choices.h\"\n",
        "\n",
        "#include <algorithm> // std::sort()\n",
        "#include <chrono>    // for timing\n",
        "#include <iostream>  // std::cout, std::endl\n",
        "\n",
        "namespace csc485b {\n",
        "namespace a1      {\n",
        "namespace cpu     {\n",
        "\n",
        "/**\n",
        " * Simple solution that just sorts the whole array with a built-in sort\n",
        " * function and then resorts the last portion in the opposing order with\n",
        " * a second call to that same built-in sort function.\n",
        " */\n",
        "void opposing_sort( element_t * data, std::size_t invert_at_pos, std::size_t num_elements )\n",
        "{\n",
        "    std::sort( data, data + num_elements, std::less< element_t >{} );\n",
        "    std::sort( data + invert_at_pos, data + num_elements, std::greater< element_t >{} );\n",
        "}\n",
        "\n",
        "/**\n",
        " * Run the single-threaded CPU baseline that students are supposed to outperform\n",
        " * in order to obtain higher grades on this assignment. Times the execution and\n",
        " * prints to the standard output (e.g., the screen) that \"wall time.\" Note that\n",
        " * the functions takes the input by value so as to not perturb the original data\n",
        " * in place.\n",
        " */\n",
        "void run_cpu_baseline( std::vector< element_t > data, std::size_t switch_at, std::size_t n )\n",
        "{\n",
        "    auto const cpu_start = std::chrono::high_resolution_clock::now();\n",
        "    opposing_sort( data.data(), switch_at, n );\n",
        "    auto const cpu_end = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    std::cout << \"CPU Baseline time: \"\n",
        "              << std::chrono::duration_cast<std::chrono::nanoseconds>(cpu_end - cpu_start).count()\n",
        "              << \" ns\" << std::endl;\n",
        "\n",
        "    for( auto const x : data ) std::cout << x << \" \"; std::cout << std::endl;\n",
        "}\n",
        "\n",
        "} // namespace cpu\n",
        "} // namespace a1\n",
        "} // namespace csc485b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "bjTbQ3EO2NwQ"
      },
      "outputs": [],
      "source": [
        "%%cuda_group_save -g \"source\" -n \"gpu_solution.cu\"\n",
        "/**\n",
        " * The file in which you will implement your GPU solutions!\n",
        " */\n",
        "\n",
        "#include \"algorithm_choices.h\"\n",
        "\n",
        "#include <chrono>    // for timing\n",
        "#include <iostream>  // std::cout, std::endl\n",
        "\n",
        "#include \"cuda_common.h\"\n",
        "\n",
        "namespace csc485b {\n",
        "namespace a1      {\n",
        "namespace gpu     {\n",
        "\n",
        "/**\n",
        " * The CPU baseline benefits from warm caches because the data was generated on\n",
        " * the CPU. Run the data through the GPU once with some arbitrary logic to\n",
        " * ensure that the GPU cache is warm too and the comparison is more fair.\n",
        " */\n",
        "__global__\n",
        "void warm_the_gpu( element_t * data, std::size_t invert_at_pos, std::size_t num_elements )\n",
        "{\n",
        "    int const th_id = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // We know this will never be true, because of the data generator logic,\n",
        "    // but I doubt that the compiler will figure it out. Thus every element\n",
        "    // should be read, but none of them should be modified.\n",
        "    if( th_id < num_elements && data[ th_id ] > num_elements * 100 )\n",
        "    {\n",
        "        ++data[ th_id ]; // should not be possible.\n",
        "    }\n",
        "}\n",
        "\n",
        "/**\n",
        " * Your solution. Should match the CPU output.\n",
        " */\n",
        "\n",
        "__global__\n",
        "void opposing_sort(element_t * data, std::size_t invert_at_pos, std::size_t num_elements)\n",
        "{\n",
        "    // Calculate the next power of 2 for the given input size\n",
        "    std::size_t padded_size = 1;\n",
        "    while (padded_size < num_elements) {\n",
        "        padded_size <<= 1;\n",
        "    }\n",
        "\n",
        "    // Step 1: Allocate shared memory with padding\n",
        "    __shared__ element_t shared_data[1024];\n",
        "\n",
        "    // Initialize padded value. They will be large values so as to finalize at the end of the sorted array\n",
        "    element_t pad_value = 1e9;\n",
        "\n",
        "    // Compute the global thread ID and local thread ID within the block\n",
        "    int const th_id = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int const local_th_id = threadIdx.x;\n",
        "\n",
        "    // Exit thread if it is not needed (ie there are fewer inputs than the possible number of threads)\n",
        "    if (local_th_id >= padded_size) return;\n",
        "\n",
        "    // Step 2: Load data from global memory to shared memory\n",
        "    if (th_id < padded_size) {\n",
        "        if (th_id < num_elements) {\n",
        "            shared_data[local_th_id] = data[th_id]; // Load actual data\n",
        "        } else {\n",
        "            shared_data[local_th_id] = pad_value; // Load padding value\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Ensure all threads in the block have loaded their data into shared memory\n",
        "    __syncthreads();\n",
        "\n",
        "    // Step 3: Perform bitonic sort using shared memory\n",
        "    // Only use the appropriate number of threads for the given array size\n",
        "    if (th_id < padded_size) {\n",
        "        // For-loop concerning each stage of the bitonic sort. log(n) stages for input of size n, where n is a power of 2\n",
        "        for (int k = 2; k <= padded_size; k <<= 1) {\n",
        "            // For-loop concerning each step of each stage. Every stage has an equivalent number of steps. i.e. stage 1 has 1 step, stage 2 has 2 steps...\n",
        "            for (int j = k >> 1; j > 0; j >>= 1) {\n",
        "\n",
        "                // Get the index of the current swap partner\n",
        "                int i_swap = local_th_id ^ j;\n",
        "\n",
        "                // If the th_id is less than the swap partner\n",
        "                if (local_th_id < i_swap && i_swap < blockDim.x) {\n",
        "                    // If we are NOT on the last stage\n",
        "                    if ((k != padded_size) || ((k == padded_size) && (padded_size != num_elements))) {\n",
        "                        // Swap the values in ascending order\n",
        "                        if ((local_th_id & k) == 0) {\n",
        "                            if (shared_data[local_th_id] > shared_data[i_swap]) {\n",
        "                                element_t t = shared_data[local_th_id];\n",
        "                                shared_data[local_th_id] = shared_data[i_swap];\n",
        "                                shared_data[i_swap] = t;\n",
        "                            }\n",
        "                        // Swap the values in descending order\n",
        "                        } else {\n",
        "                            if (shared_data[local_th_id] < shared_data[i_swap]) {\n",
        "                                element_t t = shared_data[local_th_id];\n",
        "                                shared_data[local_th_id] = shared_data[i_swap];\n",
        "                                shared_data[i_swap] = t;\n",
        "                            }\n",
        "                        }\n",
        "                    }\n",
        "                    // Handle the last step where we apply the inversion at the specified position ONLY for inputs where the size is exactly a power of 2\n",
        "                    else if (padded_size == num_elements) {\n",
        "                        if (local_th_id < invert_at_pos) {\n",
        "                            if (shared_data[local_th_id] > shared_data[i_swap]) {\n",
        "                                element_t t = shared_data[local_th_id];\n",
        "                                shared_data[local_th_id] = shared_data[i_swap];\n",
        "                                shared_data[i_swap] = t;\n",
        "                            }\n",
        "                        } else if (local_th_id >= invert_at_pos) {\n",
        "                            if (shared_data[local_th_id] < shared_data[i_swap]) {\n",
        "                                element_t t = shared_data[local_th_id];\n",
        "                                shared_data[local_th_id] = shared_data[i_swap];\n",
        "                                shared_data[i_swap] = t;\n",
        "                            }\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "\n",
        "                // Synchronize threads within the block before the next stage\n",
        "                __syncthreads();\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    // Step 3.5\n",
        "    // If the input size is not a power of 2, then perform normal bitonic sort on the array, and reverse elements after 3n/4\n",
        "    if (padded_size > num_elements) {\n",
        "        // Only use the threads between invert_at_pos and num_elements\n",
        "        if ((local_th_id >= invert_at_pos) && (local_th_id < num_elements)) {\n",
        "            // Math equation to get the appropriate swap partners\n",
        "            int swap_partner = num_elements - (local_th_id - invert_at_pos) - 1;\n",
        "            // Swap the elements using the threads\n",
        "            if (local_th_id < swap_partner) {\n",
        "                if (shared_data[local_th_id] < shared_data[swap_partner]) {\n",
        "                    element_t t = shared_data[local_th_id];\n",
        "                    shared_data[local_th_id] = shared_data[swap_partner];\n",
        "                    shared_data[swap_partner] = t;\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    // Step 4: Copy sorted data back to global memory, excluding padding\n",
        "    if (th_id < num_elements) {\n",
        "        data[th_id] = shared_data[local_th_id];\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "/**\n",
        " * Performs all the logic of allocating device vectors and copying host/input\n",
        " * vectors to the device. Times the opposing_sort() kernel with wall time,\n",
        " * but excludes set up and tear down costs such as mallocs, frees, and memcpies.\n",
        " */\n",
        "void run_gpu_soln( std::vector< element_t > data, std::size_t switch_at, std::size_t n )\n",
        "{\n",
        "    // Kernel launch configurations. Feel free to change these.\n",
        "    // This is set to maximise the size of a thread block on a T4, but it hasn't\n",
        "    // been tuned. It's not known if this is optimal.\n",
        "    std::size_t const threads_per_block = 1024;\n",
        "    std::size_t const num_blocks =  ( n + threads_per_block - 1 ) / threads_per_block;\n",
        "\n",
        "    // Allocate arrays on the device/GPU\n",
        "    element_t * d_data;\n",
        "    cudaMalloc( (void**) & d_data, sizeof( element_t ) * n );\n",
        "    CHECK_ERROR(\"Allocating input array on device\");\n",
        "\n",
        "    // Copy the input from the host to the device/GPU\n",
        "    cudaMemcpy( d_data, data.data(), sizeof( element_t ) * n, cudaMemcpyHostToDevice );\n",
        "    CHECK_ERROR(\"Copying input array to device\");\n",
        "\n",
        "    // Warm the cache on the GPU for a more fair comparison\n",
        "    warm_the_gpu<<< num_blocks, threads_per_block>>>( d_data, switch_at, n );\n",
        "\n",
        "    // Time the execution of the kernel that you implemented\n",
        "    auto const kernel_start = std::chrono::high_resolution_clock::now();\n",
        "    opposing_sort<<< num_blocks, threads_per_block>>>( d_data, switch_at, n );\n",
        "    auto const kernel_end = std::chrono::high_resolution_clock::now();\n",
        "    CHECK_ERROR(\"Executing kernel on device\");\n",
        "\n",
        "    // After the timer ends, copy the result back, free the device vector,\n",
        "    // and echo out the timings and the results.\n",
        "    cudaMemcpy( data.data(), d_data, sizeof( element_t ) * n, cudaMemcpyDeviceToHost );\n",
        "    CHECK_ERROR(\"Transferring result back to host\");\n",
        "    cudaFree( d_data );\n",
        "    CHECK_ERROR(\"Freeing device memory\");\n",
        "\n",
        "    std::cout << \"GPU Solution time: \"\n",
        "              << std::chrono::duration_cast<std::chrono::nanoseconds>(kernel_end - kernel_start).count()\n",
        "              << \" ns\" << std::endl;\n",
        "\n",
        "    for( auto const x : data ) std::cout << x << \" \"; std::cout << std::endl;\n",
        "}\n",
        "\n",
        "} // namespace gpu\n",
        "} // namespace a1\n",
        "} // namespace csc485b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "IRvVeK-QifnZ"
      },
      "outputs": [],
      "source": [
        "%%cuda_group_save -g \"source\" -n \"main.cu\"\n",
        "/**\n",
        " * Driver for the benchmark comparison. Generates random data,\n",
        " * runs the CPU baseline, and then runs your code.\n",
        " */\n",
        "\n",
        "#include <cstddef>  // std::size_t type\n",
        "#include <iostream> // std::cout, std::endl\n",
        "#include <vector>\n",
        "\n",
        "#include \"algorithm_choices.h\"\n",
        "#include \"data_generator.h\"\n",
        "#include \"data_types.h\"\n",
        "#include \"cuda_common.h\"\n",
        "\n",
        "int main()\n",
        "{\n",
        "    std::size_t const n = 576;\n",
        "    std::size_t const switch_at = 3 * ( n >> 2 ) ;\n",
        "\n",
        "    auto data = csc485b::a1::generate_uniform< element_t >( n );\n",
        "    /*\n",
        "    for (auto &d:data) {\n",
        "        std::cout << d << ' ';\n",
        "    }\n",
        "    */\n",
        "    std::cout << '\\n';\n",
        "    csc485b::a1::cpu::run_cpu_baseline( data, switch_at, n );\n",
        "    csc485b::a1::gpu::run_gpu_soln( data, switch_at, n );\n",
        "\n",
        "    return EXIT_SUCCESS;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7F0eVsGjUNp",
        "outputId": "9c370dfb-2551-41c9-bf4f-f0460e8d1919"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CPU Baseline time: 27391 ns\n",
            "1 3 7 11 12 14 17 22 22 23 23 27 27 30 30 30 33 36 38 38 40 40 47 47 48 51 56 58 58 62 63 65 65 66 67 67 69 74 79 79 81 84 84 86 87 92 95 100 101 104 111 112 113 115 116 117 119 119 119 124 130 133 134 134 136 137 142 143 144 145 146 146 146 146 149 154 154 155 155 160 161 163 166 167 169 171 174 177 177 183 183 185 187 188 188 192 194 194 194 197 199 201 209 210 214 216 217 222 222 232 234 236 239 241 242 242 246 247 249 249 249 257 261 261 263 263 265 265 265 266 270 270 272 273 273 275 280 281 284 286 288 288 289 305 306 308 309 311 312 314 314 314 315 317 318 321 323 323 324 326 329 332 332 334 335 337 342 342 342 345 346 347 349 349 350 360 362 368 372 379 381 382 382 382 384 387 387 390 391 392 393 393 394 397 399 400 401 402 404 410 410 410 414 417 418 423 424 426 426 427 428 429 431 432 438 441 442 444 445 445 446 452 453 453 455 456 458 459 459 463 465 466 467 469 470 475 479 482 482 486 486 490 490 496 497 505 505 507 509 510 519 520 522 524 528 530 530 535 535 537 537 537 547 549 551 552 557 558 560 561 562 564 565 565 568 569 570 577 577 577 579 580 584 585 590 590 591 592 594 595 597 597 598 599 602 603 603 605 607 608 609 612 616 620 621 624 628 629 629 636 636 636 639 645 649 650 653 653 656 659 659 663 665 668 674 678 682 686 687 690 691 697 698 699 701 702 702 703 707 708 709 710 713 714 717 719 722 722 724 724 726 728 730 731 732 732 737 738 741 742 744 744 745 746 747 747 749 749 749 752 753 755 755 756 756 759 759 762 763 766 771 775 777 778 780 780 781 783 784 789 792 793 795 795 796 797 801 801 802 804 805 805 807 808 808 810 810 811 812 817 819 821 822 824 825 827 830 832 834 834 835 838 839 840 840 840 842 843 843 844 847 847 1152 1151 1148 1142 1140 1140 1132 1130 1129 1127 1125 1124 1122 1121 1120 1119 1118 1117 1117 1114 1107 1106 1103 1102 1101 1100 1100 1099 1099 1095 1090 1088 1085 1083 1082 1080 1074 1071 1062 1062 1058 1055 1049 1047 1045 1044 1043 1042 1041 1038 1031 1030 1028 1028 1025 1025 1023 1022 1017 1016 1016 1009 1003 1003 1002 1001 1000 999 998 997 995 994 994 993 992 990 990 987 984 982 981 980 978 974 972 971 970 969 962 961 957 954 954 952 948 945 939 936 932 932 931 930 928 927 926 920 920 918 917 915 912 909 909 908 905 903 898 894 893 892 892 892 891 891 889 885 884 877 874 873 872 871 867 866 865 864 863 863 856 855 854 854 850 848 \n",
            "GPU Solution time: 14589 ns\n",
            "1 3 7 11 12 14 17 22 22 23 23 27 27 30 30 30 33 36 38 38 40 40 47 47 48 51 56 58 58 62 63 65 65 66 67 67 69 74 79 79 81 84 84 86 87 92 95 100 101 104 111 112 113 115 116 117 119 119 119 124 130 133 134 134 136 137 142 143 144 145 146 146 146 146 149 154 154 155 155 160 161 163 166 167 169 171 174 177 177 183 183 185 187 188 188 192 194 194 194 197 199 201 209 210 214 216 217 222 222 232 234 236 239 241 242 242 246 247 249 249 249 257 261 261 263 263 265 265 265 266 270 270 272 273 273 275 280 281 284 286 288 288 289 305 306 308 309 311 312 314 314 314 315 317 318 321 323 323 324 326 329 332 332 334 335 337 342 342 342 345 346 347 349 349 350 360 362 368 372 379 381 382 382 382 384 387 387 390 391 392 393 393 394 397 399 400 401 402 404 410 410 410 414 417 418 423 424 426 426 427 428 429 431 432 438 441 442 444 445 445 446 452 453 453 455 456 458 459 459 463 465 466 467 469 470 475 479 482 482 486 486 490 490 496 497 505 505 507 509 510 519 520 522 524 528 530 530 535 535 537 537 537 547 549 551 552 557 558 560 561 562 564 565 565 568 569 570 577 577 577 579 580 584 585 590 590 591 592 594 595 597 597 598 599 602 603 603 605 607 608 609 612 616 620 621 624 628 629 629 636 636 636 639 645 649 650 653 653 656 659 659 663 665 668 674 678 682 686 687 690 691 697 698 699 701 702 702 703 707 708 709 710 713 714 717 719 722 722 724 724 726 728 730 731 732 732 737 738 741 742 744 744 745 746 747 747 749 749 749 752 753 755 755 756 756 759 759 762 763 766 771 775 777 778 780 780 781 783 784 789 792 793 795 795 796 797 801 801 802 804 805 805 807 808 808 810 810 811 812 817 819 821 822 824 825 827 830 832 834 834 835 838 839 840 840 840 842 843 843 844 847 847 1152 1151 1148 1142 1140 1140 1132 1130 1129 1127 1125 1124 1122 1121 1120 1119 1118 1117 1117 1114 1107 1106 1103 1102 1101 1100 1100 1099 1099 1095 1090 1088 1085 1083 1082 1080 1074 1071 1062 1062 1058 1055 1049 1047 1045 1044 1043 1042 1041 1038 1031 1030 1028 1028 1025 1025 1023 1022 1017 1016 1016 1009 1003 1003 1002 1001 1000 999 998 997 995 994 994 993 992 990 990 987 984 982 981 980 978 974 972 971 970 969 962 961 957 954 954 952 948 945 939 936 932 932 931 930 928 927 926 920 920 918 917 915 912 909 909 908 905 903 898 894 893 892 892 892 891 891 889 885 884 877 874 873 872 871 867 866 865 864 863 863 856 855 854 854 850 848 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "%cuda_group_run --group \"source\" --compiler-args \"-O3 -g -std=c++20 -arch=sm_75\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "K0Yqomwu6WsP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30776,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}