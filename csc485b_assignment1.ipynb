{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Load the extension that allows us to compile CUDA code in python notebooks\n# Documentation is here: https://nvcc4jupyter.readthedocs.io/en/latest/\n!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git\n%load_ext nvcc4jupyter","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dclGnLGAgbtH","outputId":"5d326cc4-09ee-4f40-bded-3b3c2e2f3d94","execution":{"iopub.status.busy":"2024-09-25T01:36:32.278988Z","iopub.execute_input":"2024-09-25T01:36:32.279371Z","iopub.status.idle":"2024-09-25T01:38:27.798008Z","shell.execute_reply.started":"2024-09-25T01:36:32.279333Z","shell.execute_reply":"2024-09-25T01:38:27.796734Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-2kesvh7_\n  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-2kesvh7_\n  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit 28f872a2f99a1b201bcd0db14fdbc5a496b9bfd7\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: nvcc4jupyter\n  Building wheel for nvcc4jupyter (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for nvcc4jupyter: filename=nvcc4jupyter-1.2.1-py3-none-any.whl size=10743 sha256=3a570d3ab74e9051a615aa78d54722c78a69a0c83a1d407fccde14c37a30e96c\n  Stored in directory: /tmp/pip-ephem-wheel-cache-jr_42pl_/wheels/a8/b9/18/23f8ef71ceb0f63297dd1903aedd067e6243a68ea756d6feea\nSuccessfully built nvcc4jupyter\nInstalling collected packages: nvcc4jupyter\nSuccessfully installed nvcc4jupyter-1.2.1\nDetected platform \"Kaggle\". Running its setup...\nUpdating the package lists...\nInstalling nvidia-cuda-toolkit, this may take a few minutes...\nSource files will be saved in \"/tmp/tmp10mt_c7v\".\n","output_type":"stream"}]},{"cell_type":"code","source":"%%cuda_group_save -g \"source\" -n \"data_types.h\"\n/**\n * A collection of commonly used data types throughout this project.\n */\n#pragma once\n\n#include <stdint.h> // uint32_t\n\nusing element_t = uint32_t;","metadata":{"id":"VVbDQthwogQF","execution":{"iopub.status.busy":"2024-09-25T01:38:27.800315Z","iopub.execute_input":"2024-09-25T01:38:27.801320Z","iopub.status.idle":"2024-09-25T01:38:27.807221Z","shell.execute_reply.started":"2024-09-25T01:38:27.801281Z","shell.execute_reply":"2024-09-25T01:38:27.806177Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"%%cuda_group_save -g \"source\" -n \"cuda_common.h\"\n/**\n * Standard macros that can be useful for error checking.\n * https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__ERROR.html\n */\n#pragma once\n\n#include <cuda.h>\n\n#define CUDA_CALL(exp)                                       \\\n    do {                                                     \\\n        cudaError res = (exp);                               \\\n        if(res != cudaSuccess) {                             \\\n            printf(\"Error at %s:%d\\n %s\\n\",                  \\\n                __FILE__,__LINE__, cudaGetErrorString(res)); \\\n           exit(EXIT_FAILURE);                               \\\n        }                                                    \\\n    } while(0)\n\n#define CHECK_ERROR(msg)                                             \\\n    do {                                                             \\\n        cudaError_t err = cudaGetLastError();                        \\\n        if(cudaSuccess != err) {                                     \\\n            printf(\"Error (%s) at %s:%d\\n %s\\n\",                     \\\n                (msg), __FILE__, __LINE__, cudaGetErrorString(err)); \\\n            exit(EXIT_FAILURE);                                      \\\n        }                                                            \\\n    } while (0)","metadata":{"id":"ZqET4uI2ggwf","execution":{"iopub.status.busy":"2024-09-25T01:38:27.808957Z","iopub.execute_input":"2024-09-25T01:38:27.809349Z","iopub.status.idle":"2024-09-25T01:38:28.497615Z","shell.execute_reply.started":"2024-09-25T01:38:27.809297Z","shell.execute_reply":"2024-09-25T01:38:28.496762Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"%%cuda_group_save -g \"source\" -n \"data_generator.h\"\n/**\n * Functions for generating random input data with a fixed seed\n */\n#pragma once\n\n#include <random>  // for std::mt19937, std::uniform_int_distribution\n#include <vector>\n\n#include \"data_types.h\"\n\nnamespace csc485b {\nnamespace a1 {\n\n/**\n * Generates and returns a vector of random uniform data of a given length, n,\n * for any integral type. Input range will be [0, 2n].\n */\ntemplate < typename T >\nstd::vector< T > generate_uniform( std::size_t n )\n{\n    // for details of random number generation, see:\n    // https://en.cppreference.com/w/cpp/numeric/random/uniform_int_distribution\n    std::size_t random_seed = 20240916;  // use magic seed\n    std::mt19937 rng( random_seed );     // use mersenne twister generator\n    std::uniform_int_distribution<> distrib(0, 2 * n);\n\n    std::vector< T > random_data( n ); // init array\n    std::generate( std::begin( random_data )\n                 , std::end  ( random_data )\n                 , [ &rng, &distrib ](){ return static_cast< T >( distrib( rng ) ); });\n\n    return random_data;\n}\n\n} // namespace a1\n} // namespace csc485b","metadata":{"id":"GY0L7rKhoVaZ","execution":{"iopub.status.busy":"2024-09-25T01:38:28.500057Z","iopub.execute_input":"2024-09-25T01:38:28.500670Z","iopub.status.idle":"2024-09-25T01:38:28.510639Z","shell.execute_reply.started":"2024-09-25T01:38:28.500624Z","shell.execute_reply":"2024-09-25T01:38:28.509855Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"%%cuda_group_save -g \"source\" -n \"algorithm_choices.h\"\n#pragma once\n\n#include <vector>\n\n#include \"data_types.h\"\n\nnamespace csc485b {\nnamespace a1 {\nnamespace cpu {\n\nvoid run_cpu_baseline( std::vector< element_t > data, std::size_t switch_at, std::size_t n );\n\n} // namespace cpu\n\n\nnamespace gpu {\n\nvoid run_gpu_soln( std::vector< element_t > data, std::size_t switch_at, std::size_t n );\n\n} // namespace gpu\n} // namespace a1\n} // namespace csc485b","metadata":{"id":"IJOKRZuCkDh2","execution":{"iopub.status.busy":"2024-09-25T01:38:28.511791Z","iopub.execute_input":"2024-09-25T01:38:28.512186Z","iopub.status.idle":"2024-09-25T01:38:28.524970Z","shell.execute_reply.started":"2024-09-25T01:38:28.512145Z","shell.execute_reply":"2024-09-25T01:38:28.524176Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"%%cuda_group_save -g \"source\" -n \"cpu_baseline.cu\"\n/**\n * CPU methods that the GPU should outperform.\n */\n\n#include \"algorithm_choices.h\"\n\n#include <algorithm> // std::sort()\n#include <chrono>    // for timing\n#include <iostream>  // std::cout, std::endl\n\nnamespace csc485b {\nnamespace a1      {\nnamespace cpu     {\n\n/**\n * Simple solution that just sorts the whole array with a built-in sort\n * function and then resorts the last portion in the opposing order with\n * a second call to that same built-in sort function.\n */\nvoid opposing_sort( element_t * data, std::size_t invert_at_pos, std::size_t num_elements )\n{\n    std::sort( data, data + num_elements, std::less< element_t >{} );\n    std::sort( data + invert_at_pos, data + num_elements, std::greater< element_t >{} );\n}\n\n/**\n * Run the single-threaded CPU baseline that students are supposed to outperform\n * in order to obtain higher grades on this assignment. Times the execution and\n * prints to the standard output (e.g., the screen) that \"wall time.\" Note that\n * the functions takes the input by value so as to not perturb the original data\n * in place.\n */\nvoid run_cpu_baseline( std::vector< element_t > data, std::size_t switch_at, std::size_t n )\n{\n    auto const cpu_start = std::chrono::high_resolution_clock::now();\n    opposing_sort( data.data(), switch_at, n );\n    auto const cpu_end = std::chrono::high_resolution_clock::now();\n\n    std::cout << \"CPU Baseline time: \"\n              << std::chrono::duration_cast<std::chrono::nanoseconds>(cpu_end - cpu_start).count()\n              << \" ns\" << std::endl;\n\n    for( auto const x : data ) std::cout << x << \" \"; std::cout << std::endl;\n}\n\n} // namespace cpu\n} // namespace a1\n} // namespace csc485b","metadata":{"id":"V3lAuiBEhKjc","execution":{"iopub.status.busy":"2024-09-25T01:38:28.526034Z","iopub.execute_input":"2024-09-25T01:38:28.526320Z","iopub.status.idle":"2024-09-25T01:38:28.537142Z","shell.execute_reply.started":"2024-09-25T01:38:28.526289Z","shell.execute_reply":"2024-09-25T01:38:28.536150Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"%%cuda_group_save -g \"source\" -n \"gpu_solution.cu\"\n/**\n * The file in which you will implement your GPU solutions!\n */\n\n#include \"algorithm_choices.h\"\n\n#include <chrono>    // for timing\n#include <iostream>  // std::cout, std::endl\n\n#include \"cuda_common.h\"\n\nnamespace csc485b {\nnamespace a1      {\nnamespace gpu     {\n\n/**\n * The CPU baseline benefits from warm caches because the data was generated on\n * the CPU. Run the data through the GPU once with some arbitrary logic to\n * ensure that the GPU cache is warm too and the comparison is more fair.\n */\n__global__\nvoid warm_the_gpu( element_t * data, std::size_t invert_at_pos, std::size_t num_elements )\n{\n    int const th_id = blockIdx.x * blockDim.x + threadIdx.x;\n\n    // We know this will never be true, because of the data generator logic,\n    // but I doubt that the compiler will figure it out. Thus every element\n    // should be read, but none of them should be modified.\n    if( th_id < num_elements && data[ th_id ] > num_elements * 100 )\n    {\n        ++data[ th_id ]; // should not be possible.\n    }\n}\n\n/**\n * Your solution. Should match the CPU output.\n */\n__global__\nvoid opposing_sort( element_t * data, std::size_t invert_at_pos, std::size_t num_elements )\n{\n    // This will be the for-loop for the each STEP of the algorithm. (Rememeber a step is comprised of stages, in the graphic)\n      // k will be equal to 2 to the power of whatever step were on\n      // std::cout << \"Value pointed to by data: \" << data << std::endl;\n      int const th_id = blockIdx.x * blockDim.x + threadIdx.x;\n    if (th_id < num_elements) {\n      for (int k = 2; k <= num_elements; k = k << 1) {\n\n          // This will be the for-loop for each STAGE of each step\n          // j will be equal to k/2, and divide itself by 2 each time.\n          // For example, if we are on step 3 of a 16-length array (4 steps total)\n          // Then k = 8, j = 4, then 2 then 1, which is the total number of comparisons for each stage\n          for (int j = k >> 1; j > 0; j = j >> 1) {\n              // inside here is where every comparison will take place by the threads\n\n              int i_swap = th_id^j;\n\n              // By performing the XOR, we are basically saying that the sorting partner (or swap partner) of the thread we are on, will be indexed\n              // at that thread plus j distance away.\n              // so if th_ids are 0, 1, 2, 3\n              // and we are on stage one step one, so k=2, and j = 1 because there is 1 stage in step 1\n              // then the first swapping partner will be 0 and 0+1=1\n              // then take the xor of th_id 1 and j=1 which is 0. This is the same as what we just said above.\n              // Next take th_id 2 and xor 1 is 2+1=3, so indexes 2 and 3 are swapping partners.\n              // To make sure again, do the next step which is th_id 3 and j = 1. and 3 xor 1 is 2.\n\n              // In this solution, we will only be finding (or considering) the swapping partner for the lower of the each swapping pair,\n              // because that halfs the instructions we have to do:\n              if ((th_id < i_swap)) {\n\n                  // Here we are doing the usual bitonic sort, until the very last step\n                  if (k != num_elements) {\n\n                      // Now if we are on the first step for example, k = 2, which means every 2 elements will change bitonic direction\n                      // Once we are on the last step, k will equal the size of the array, meaning there will only be one bitonic direction and the\n                      // list will be sorted\n\n                      if ((th_id&k) == 0) {\n                          // swap in ascending order\n                          if (data[th_id] > data[i_swap]) {\n                              element_t t = data[th_id];\n                              data[th_id] = data[i_swap];\n                              data[i_swap] = t;\n                          }\n                          \n                      }\n                      else {\n                          //swap in descending order\n                          if (data[i_swap] > data[th_id]) {\n                              element_t t = data[th_id];\n                              data[th_id] = data[i_swap];\n                              data[i_swap] = t;\n                          }\n                      }\n                  }\n                  // Here we are in the last step, so we have to handle the bitonic sort quirk for the assignment which requires everything\n                  // After invert_at_pos to be in descending order. This is done in the last step, because there are no more bitonic\n                  // sequences in the last step, meaning everything would otherwise be switched into ascending order.\n                  \n                  else {\n                      // Sort everything up to invert_at_pos in ascending order\n                      if (th_id < invert_at_pos) {\n                        if (data[th_id] > data[i_swap]) {\n                            // Swap in ascending order\n                            element_t t = data[th_id];\n                            data[th_id] = data[i_swap];\n                            data[i_swap] = t;\n                        }\n                      }\n                      // Sort everything after invert_at_pos in descending order\n                      else if (th_id >= invert_at_pos) {\n                          if (data[th_id] < data[i_swap]) {\n                              // Swap in descending order\n                              element_t t = data[th_id];\n                              data[th_id] = data[i_swap];\n                              data[i_swap] = t;\n                          }\n                      }\n                  }\n                  \n              }\n          }\n      }\n        \n    }\n      return;\n}\n\n/**\n * Performs all the logic of allocating device vectors and copying host/input\n * vectors to the device. Times the opposing_sort() kernel with wall time,\n * but excludes set up and tear down costs such as mallocs, frees, and memcpies.\n */\nvoid run_gpu_soln( std::vector< element_t > data, std::size_t switch_at, std::size_t n )\n{\n    // Kernel launch configurations. Feel free to change these.\n    // This is set to maximise the size of a thread block on a T4, but it hasn't\n    // been tuned. It's not known if this is optimal.\n    std::size_t const threads_per_block = 1024;\n    std::size_t const num_blocks =  ( n + threads_per_block - 1 ) / threads_per_block;\n\n    // Allocate arrays on the device/GPU\n    element_t * d_data;\n    cudaMalloc( (void**) & d_data, sizeof( element_t ) * n );\n    CHECK_ERROR(\"Allocating input array on device\");\n\n    // Copy the input from the host to the device/GPU\n    cudaMemcpy( d_data, data.data(), sizeof( element_t ) * n, cudaMemcpyHostToDevice );\n    CHECK_ERROR(\"Copying input array to device\");\n\n    // Warm the cache on the GPU for a more fair comparison\n    warm_the_gpu<<< num_blocks, threads_per_block>>>( d_data, switch_at, n );\n\n    // Time the execution of the kernel that you implemented\n    auto const kernel_start = std::chrono::high_resolution_clock::now();\n    opposing_sort<<< num_blocks, threads_per_block>>>( d_data, switch_at, n );\n    auto const kernel_end = std::chrono::high_resolution_clock::now();\n    CHECK_ERROR(\"Executing kernel on device\");\n\n    // After the timer ends, copy the result back, free the device vector,\n    // and echo out the timings and the results.\n    cudaMemcpy( data.data(), d_data, sizeof( element_t ) * n, cudaMemcpyDeviceToHost );\n    CHECK_ERROR(\"Transferring result back to host\");\n    cudaFree( d_data );\n    CHECK_ERROR(\"Freeing device memory\");\n\n    std::cout << \"GPU Solution time: \"\n              << std::chrono::duration_cast<std::chrono::nanoseconds>(kernel_end - kernel_start).count()\n              << \" ns\" << std::endl;\n\n    for( auto const x : data ) std::cout << x << \" \"; std::cout << std::endl;\n}\n\n} // namespace gpu\n} // namespace a1\n} // namespace csc485b","metadata":{"id":"bjTbQ3EO2NwQ","execution":{"iopub.status.busy":"2024-09-25T02:44:53.085021Z","iopub.execute_input":"2024-09-25T02:44:53.085361Z","iopub.status.idle":"2024-09-25T02:44:53.095536Z","shell.execute_reply.started":"2024-09-25T02:44:53.085328Z","shell.execute_reply":"2024-09-25T02:44:53.094665Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"%%cuda_group_save -g \"source\" -n \"main.cu\"\n/**\n * Driver for the benchmark comparison. Generates random data,\n * runs the CPU baseline, and then runs your code.\n */\n\n#include <cstddef>  // std::size_t type\n#include <iostream> // std::cout, std::endl\n#include <vector>\n\n#include \"algorithm_choices.h\"\n#include \"data_generator.h\"\n#include \"data_types.h\"\n#include \"cuda_common.h\"\n\nint main()\n{\n    std::size_t const n = 32;\n    std::size_t const switch_at = 3 * ( n >> 2 ) ;\n\n    auto data = csc485b::a1::generate_uniform< element_t >( n );\n    /*\n    for (auto &d:data) {\n        std::cout << d << ' ';\n    }\n    */\n    std::cout << '\\n';\n    csc485b::a1::cpu::run_cpu_baseline( data, switch_at, n );\n    csc485b::a1::gpu::run_gpu_soln( data, switch_at, n );\n\n    return EXIT_SUCCESS;\n}","metadata":{"id":"IRvVeK-QifnZ","execution":{"iopub.status.busy":"2024-09-25T02:48:31.322534Z","iopub.execute_input":"2024-09-25T02:48:31.322932Z","iopub.status.idle":"2024-09-25T02:48:31.328931Z","shell.execute_reply.started":"2024-09-25T02:48:31.322895Z","shell.execute_reply":"2024-09-25T02:48:31.327970Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"%cuda_group_run --group \"source\" --compiler-args \"-O0 -g -std=c++20 -arch=sm_75\"","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S7F0eVsGjUNp","outputId":"310fd1bb-bd82-4238-d721-f6637f546255","execution":{"iopub.status.busy":"2024-09-25T02:48:32.865746Z","iopub.execute_input":"2024-09-25T02:48:32.866631Z","iopub.status.idle":"2024-09-25T02:48:42.674324Z","shell.execute_reply.started":"2024-09-25T02:48:32.866587Z","shell.execute_reply":"2024-09-25T02:48:42.673351Z"},"trusted":true},"execution_count":98,"outputs":[{"name":"stdout","text":"\nCPU Baseline time: 4392 ns\n0 0 1 1 6 7 8 10 10 10 11 13 13 13 15 19 19 23 27 27 31 33 34 35 59 57 56 52 52 51 39 36 \nGPU Solution time: 12979 ns\n0 0 1 1 6 7 8 10 10 10 11 13 13 13 15 19 19 23 27 27 31 33 34 35 59 57 56 52 52 51 39 36 \n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"id":"K0Yqomwu6WsP"},"execution_count":null,"outputs":[]}]}